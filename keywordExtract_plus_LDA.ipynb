{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keywordExtract.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JedE2T3lGNJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c82404-d107-4705-c3ca-6f00eafe91a6"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_275\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)\n",
            "Collecting pyspark==2.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 62kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 43.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130389 sha256=fce62f6b8ebcacd993b1cc141d6ebf473db08520747a19d1ca9d16ae4aa586a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "Collecting spark-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/84/3f15673db521fbc4e8e0ec3677a019ba1458b2cb70f0f7738c221511ef32/spark_nlp-2.6.3-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4O0ukvcGMu1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "56b00fc7-5c4d-4619-f894-db6f992af907"
      },
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version\")\n",
        "sparknlp.version()\n",
        "print(\"Apache Spark version\")\n",
        "spark.version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version\n",
            "Apache Spark version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UpWXXfUGEWq"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import col, lit\n",
        "from functools import reduce\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from pyspark.ml.feature import CountVectorizer, MinHashLSH\n",
        "from pyspark.mllib.clustering import LDA, LDAModel\n",
        "from pyspark.mllib.linalg import Vector, Vectors\n",
        "# from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "conf=SparkConf().setAppName(\"miniProject\").setMaster(\"local[*]\")\n",
        "sc=SparkContext.getOrCreate(conf)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhv8MnibEZe5"
      },
      "source": [
        "# def insert2mongodb_2(df):\n",
        "#     mongoClient = MongoClient(\"mongodb://hkust:hkustAb$13gid@52.229.166.95:27017/\")\n",
        "#     db = mongoClient[\"MSBD5003\"]\n",
        "\n",
        "\n",
        "#     # connection = MongoClient(\"mongodb://msbd5003-db-server.eastasia.cloudapp.azure.com:27017/\")\n",
        "#     # db = connection['MSBD5003']\n",
        "#     # db.authenticate(\"hkust\", \"hkustAb$13gid\")\n",
        "#     All = []\n",
        "#     collection = db.tweets\n",
        "#     for index, row in df.iterrows():\n",
        "#         if(row['language'] == 'en'):\n",
        "#             tweet_dic = {'id': row['id'], 'text':row['tweet'],'created':row['date']}\n",
        "#             All.append(tweet_dic)\n",
        "#     #         tweet_dic = {'text':row['text'],'created':row['date']}\n",
        "#     collection.insert_many(All)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0jUwj-XHnsi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "540f61c0-68cd-4c42-aae1-e69777c26d10"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/archive-3/2.csv\",nrows=1000)\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserName</th>\n",
              "      <th>ScreenName</th>\n",
              "      <th>Location</th>\n",
              "      <th>TweetAt</th>\n",
              "      <th>OriginalTweet</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3799</td>\n",
              "      <td>48751</td>\n",
              "      <td>London</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3800</td>\n",
              "      <td>48752</td>\n",
              "      <td>UK</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>advice Talk to your neighbours family to excha...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3801</td>\n",
              "      <td>48753</td>\n",
              "      <td>Vagabonds</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3802</td>\n",
              "      <td>48754</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>My food stock is not the only one which is emp...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3803</td>\n",
              "      <td>48755</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16-03-2020</td>\n",
              "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
              "      <td>Extremely Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserName  ...           Sentiment\n",
              "0      3799  ...             Neutral\n",
              "1      3800  ...            Positive\n",
              "2      3801  ...            Positive\n",
              "3      3802  ...            Positive\n",
              "4      3803  ...  Extremely Negative\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An9kThWTH7j_"
      },
      "source": [
        "def removecharacters(text):   \n",
        "    text=text.strip()\n",
        "    text = re.sub('\\s+', ' ', text).strip()\n",
        "    return text\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAdz_sSDNEt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65122636-c399-492c-b5c4-7bb5f291f472"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "nltk.download('stopwords')\n",
        "sw = stopwords.words('english')\n",
        "word_tokenizer = TweetTokenizer(strip_handles=True, preserve_case=False, reduce_len=True)\n",
        "def remove_punc_and_stopword(text):\n",
        "    punc_removed = [word for word in text if word not in string.punctuation]\n",
        "    punc_removed = ''.join(punc_removed)\n",
        "    punc_removed = word_tokenizer.tokenize(punc_removed)\n",
        "    vocabularies = []\n",
        "    for token in punc_removed :\n",
        "        if token.lower() not in sw:\n",
        "            vocabularies.append(token.lower())\n",
        "    return vocabularies"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzHIv3NVN2e3"
      },
      "source": [
        "def rem_single_characters_and_http(lst):\n",
        "    outputlst = []\n",
        "    for word in lst:\n",
        "      if word.startswith(\"http\") == False:\n",
        "        temp = re.sub('[^a-zA-Z ]+',' ', word) \n",
        "        temp=re.sub(\"&lt;/?.*?&gt;\",' ',temp)\n",
        "        temp=re.sub(\"(\\\\d|\\\\W)+\",\" \",temp)\n",
        "        if(len(temp)<=3):\n",
        "            outputlst.append(' ')        \n",
        "        else:\n",
        "            outputlst.append(temp)\n",
        "    return outputlst"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHPqagoKPsQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e57728-948b-425e-82db-a0461d9be2bb"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "filter_words = ['covid']\n",
        "def lemmatizationFunct(x):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    finalLem = []\n",
        "    for s in x:\n",
        "      vab = lemmatizer.lemmatize(s)\n",
        "      if vab.startswith(\" \") == False and vab not in filter_words:\n",
        "        finalLem.append(vab)\n",
        "    return finalLem"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P38_cNNRQi_Q"
      },
      "source": [
        "def joinTokensFunct(x):\n",
        "    x = \" \".join(x)\n",
        "    return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF1YSPDLQyfi"
      },
      "source": [
        "def removecharacters(text):   \n",
        "    text=text.strip()\n",
        "    text = re.sub('\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wme8e5-5IK6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93439aca-70e0-4ab0-80f4-69b98007698b"
      },
      "source": [
        "lines = df['OriginalTweet'].values.tolist()\n",
        "print(lines[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8', 'advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order', 'Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P', \"My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\", \"Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\", 'As news of the region\\x92s first confirmed COVID-19 case came out of Sullivan County last week, people flocked to area stores to purchase cleaning supplies, hand sanitizer, food, toilet paper and other goods, @Tim_Dodson reports https://t.co/cfXch7a2lU', 'Cashier at grocery store was sharing his insights on #Covid_19 To prove his credibility he commented \"I\\'m in Civics class so I know what I\\'m talking about\". https://t.co/ieFDNeHgDO', \"Was at the supermarket today. Didn't buy toilet paper. #Rebel\\r\\r\\n\\r\\r\\n#toiletpapercrisis #covid_19 https://t.co/eVXkQLIdAZ\", 'Due to COVID-19 our retail store and classroom in Atlanta will not be open for walk-in business or classes for the next two weeks, beginning Monday, March 16.  We will continue to process online and phone orders as normal! Thank you for your understanding! https://t.co/kw91zJ5O5i', \"For corona prevention,we should stop to buy things with the cash and should use online payment methods because corona can spread through the notes. Also we should prefer online shopping from our home. It's time to fight against COVID 19?. #govindia #IndiaFightsCorona\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pjqABy8LExP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59b1831-9e77-480b-882b-f340cad25ecd"
      },
      "source": [
        "RDD = sc.parallelize(lines)\n",
        "RDD.take(10)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8',\n",
              " 'advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order',\n",
              " 'Coronavirus Australia: Woolworths to give elderly, disabled dedicated shopping hours amid COVID-19 outbreak https://t.co/bInCA9Vp8P',\n",
              " \"My food stock is not the only one which is empty...\\r\\r\\n\\r\\r\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\r\\r\\nStay calm, stay safe.\\r\\r\\n\\r\\r\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\",\n",
              " \"Me, ready to go at supermarket during the #COVID19 outbreak.\\r\\r\\n\\r\\r\\nNot because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\\r\\r\\n\\r\\r\\n#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\",\n",
              " 'As news of the region\\x92s first confirmed COVID-19 case came out of Sullivan County last week, people flocked to area stores to purchase cleaning supplies, hand sanitizer, food, toilet paper and other goods, @Tim_Dodson reports https://t.co/cfXch7a2lU',\n",
              " 'Cashier at grocery store was sharing his insights on #Covid_19 To prove his credibility he commented \"I\\'m in Civics class so I know what I\\'m talking about\". https://t.co/ieFDNeHgDO',\n",
              " \"Was at the supermarket today. Didn't buy toilet paper. #Rebel\\r\\r\\n\\r\\r\\n#toiletpapercrisis #covid_19 https://t.co/eVXkQLIdAZ\",\n",
              " 'Due to COVID-19 our retail store and classroom in Atlanta will not be open for walk-in business or classes for the next two weeks, beginning Monday, March 16.  We will continue to process online and phone orders as normal! Thank you for your understanding! https://t.co/kw91zJ5O5i',\n",
              " \"For corona prevention,we should stop to buy things with the cash and should use online payment methods because corona can spread through the notes. Also we should prefer online shopping from our home. It's time to fight against COVID 19?. #govindia #IndiaFightsCorona\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSMS1ogeLeMt"
      },
      "source": [
        "LC_vab_RDD = RDD.map(remove_punc_and_stopword)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiyB7HYXNbfa"
      },
      "source": [
        "rem_RDD = LC_vab_RDD.map(rem_single_characters_and_http)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdNdo-_oN_IE"
      },
      "source": [
        "lem_wordsRDD = rem_RDD.map(lemmatizationFunct)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edLsLCRRPy18"
      },
      "source": [
        "joinedTokens = lem_wordsRDD.map(joinTokensFunct)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdr9yfh5QgYW"
      },
      "source": [
        "rem_spacesRDD = joinedTokens.map(removecharacters)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUgkQtNRS3qI"
      },
      "source": [
        "wordcounts = rem_spacesRDD.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\\\n",
        "             .sortBy(lambda x : x[1],False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhTS5xAtW0kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1cbdc3b-b834-4c51-c016-4ecfbcada363"
      },
      "source": [
        "wordcounts.take(30)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('coronavirus', 408),\n",
              " ('store', 304),\n",
              " ('food', 288),\n",
              " ('grocery', 192),\n",
              " ('people', 185),\n",
              " ('supermarket', 165),\n",
              " ('panic', 144),\n",
              " ('shopping', 141),\n",
              " ('online', 133),\n",
              " ('consumer', 117),\n",
              " ('need', 111),\n",
              " ('retail', 97),\n",
              " ('stock', 96),\n",
              " ('price', 87),\n",
              " ('buying', 86),\n",
              " ('please', 76),\n",
              " ('time', 73),\n",
              " ('work', 69),\n",
              " ('supply', 69),\n",
              " ('home', 68),\n",
              " ('going', 64),\n",
              " ('like', 63),\n",
              " ('shelf', 58),\n",
              " ('worker', 52),\n",
              " ('help', 51),\n",
              " ('pandemic', 51),\n",
              " ('employee', 49),\n",
              " ('week', 48),\n",
              " ('shop', 48),\n",
              " ('customer', 46)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct-QHKTTYs6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c4ec43-2315-4943-fe03-635cd0e8e743"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "final = rem_spacesRDD.map(lambda x: [x])\n",
        "df = spark.createDataFrame(final).toDF(\"text\")\n",
        "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
        "df.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---+\n",
            "|                text| id|\n",
            "+--------------------+---+\n",
            "|menyrbie philgaha...|  0|\n",
            "|advice talk neigh...|  1|\n",
            "|coronavirus austr...|  2|\n",
            "|food stock empty ...|  3|\n",
            "|ready supermarket...|  4|\n",
            "|news region first...|  5|\n",
            "|cashier grocery s...|  6|\n",
            "|supermarket today...|  7|\n",
            "|retail store clas...|  8|\n",
            "|corona prevention...|  9|\n",
            "|month hasnt crowd...| 10|\n",
            "|situation increas...| 11|\n",
            "|horningsea caring...| 12|\n",
            "|dont need stock f...| 13|\n",
            "|adara release res...| 14|\n",
            "|line grocery stor...| 15|\n",
            "|                    | 16|\n",
            "|eyeonthearctic ru...| 17|\n",
            "|amazon glitch sty...| 18|\n",
            "|arent struggling ...| 19|\n",
            "+--------------------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QmJZG2zZIME"
      },
      "source": [
        "from pyspark.ml.feature import NGram"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTueFF0jgA_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0999ff33-2626-4c02-8bb7-9b139b9f49b7"
      },
      "source": [
        "finaldf2 = df.withColumn(\"words\", F.split(df['text'], ' '))\n",
        "finaldf2.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---+--------------------+\n",
            "|                text| id|               words|\n",
            "+--------------------+---+--------------------+\n",
            "|menyrbie philgaha...|  0|[menyrbie, philga...|\n",
            "|advice talk neigh...|  1|[advice, talk, ne...|\n",
            "|coronavirus austr...|  2|[coronavirus, aus...|\n",
            "|food stock empty ...|  3|[food, stock, emp...|\n",
            "|ready supermarket...|  4|[ready, supermark...|\n",
            "|news region first...|  5|[news, region, fi...|\n",
            "|cashier grocery s...|  6|[cashier, grocery...|\n",
            "|supermarket today...|  7|[supermarket, tod...|\n",
            "|retail store clas...|  8|[retail, store, c...|\n",
            "|corona prevention...|  9|[corona, preventi...|\n",
            "|month hasnt crowd...| 10|[month, hasnt, cr...|\n",
            "|situation increas...| 11|[situation, incre...|\n",
            "|horningsea caring...| 12|[horningsea, cari...|\n",
            "|dont need stock f...| 13|[dont, need, stoc...|\n",
            "|adara release res...| 14|[adara, release, ...|\n",
            "|line grocery stor...| 15|[line, grocery, s...|\n",
            "|                    | 16|                  []|\n",
            "|eyeonthearctic ru...| 17|[eyeonthearctic, ...|\n",
            "|amazon glitch sty...| 18|[amazon, glitch, ...|\n",
            "|arent struggling ...| 19|[arent, strugglin...|\n",
            "+--------------------+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHTL99_EhOGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04c7326-272b-4a1f-a256-1f058ad47f5a"
      },
      "source": [
        "bigram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
        "bigramDataFrame = bigram.transform(finaldf2)\n",
        "bigramDataFrame.show(10)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---+--------------------+--------------------+\n",
            "|                text| id|               words|             bigrams|\n",
            "+--------------------+---+--------------------+--------------------+\n",
            "|menyrbie philgaha...|  0|[menyrbie, philga...|[menyrbie philgah...|\n",
            "|advice talk neigh...|  1|[advice, talk, ne...|[advice talk, tal...|\n",
            "|coronavirus austr...|  2|[coronavirus, aus...|[coronavirus aust...|\n",
            "|food stock empty ...|  3|[food, stock, emp...|[food stock, stoc...|\n",
            "|ready supermarket...|  4|[ready, supermark...|[ready supermarke...|\n",
            "|news region first...|  5|[news, region, fi...|[news region, reg...|\n",
            "|cashier grocery s...|  6|[cashier, grocery...|[cashier grocery,...|\n",
            "|supermarket today...|  7|[supermarket, tod...|[supermarket toda...|\n",
            "|retail store clas...|  8|[retail, store, c...|[retail store, st...|\n",
            "|corona prevention...|  9|[corona, preventi...|[corona preventio...|\n",
            "+--------------------+---+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h31VAzjhb7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197cd5b3-8219-4322-ecb3-7d2ca815832c"
      },
      "source": [
        "topbigrams = bigramDataFrame.withColumn('topbigrams', F.explode(F.col('bigrams')))\\\n",
        "    .groupBy('topbigrams')\\\n",
        "    .count()\\\n",
        "    .sort('count', ascending=False)\n",
        "topbigrams.show(20)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|          topbigrams|count|\n",
            "+--------------------+-----+\n",
            "|       grocery store|  138|\n",
            "|     online shopping|   72|\n",
            "|        panic buying|   60|\n",
            "|        retail store|   35|\n",
            "|        toilet paper|   34|\n",
            "|          stock food|   23|\n",
            "|         food supply|   20|\n",
            "|           food bank|   18|\n",
            "|     shopping online|   16|\n",
            "|coronavirus outbreak|   15|\n",
            "|           stay home|   14|\n",
            "|        supply chain|   13|\n",
            "|coronavirus coron...|   12|\n",
            "|   social distancing|   12|\n",
            "|    grocery shopping|   12|\n",
            "|coronavirusoutbre...|   12|\n",
            "|       food shortage|   11|\n",
            "|         empty shelf|   11|\n",
            "|      store employee|   11|\n",
            "|   supermarket shelf|   10|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYlbSTkEhuA1"
      },
      "source": [
        "# trigram = NGram(n=3, inputCol=\"words\", outputCol=\"trigrams\")\n",
        "# trigramDataFrame = trigram.transform(bigramDataFrame)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArO5dGjlJ6-i"
      },
      "source": [
        "# topwords = trigramDataFrame.withColumn('word', F.explode(F.col('trigrams')))\\\n",
        "#     .groupBy('word')\\\n",
        "#     .count()\\\n",
        "#     .sort('count', ascending=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paSzNwjXKCOQ"
      },
      "source": [
        "# topwords.show()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUQzSpbbKIQ5"
      },
      "source": [
        "# topwords.write.csv('topwords_trigrams1.csv')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_gnxzscKR7x"
      },
      "source": [
        "# list_trigrams = topwords.select(\"word\").rdd.flatMap(lambda x: x)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7hSOg5PKVgn"
      },
      "source": [
        "# list_trigrams.take(20)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyEdilGNFibU"
      },
      "source": [
        "from pyspark.ml.feature import Normalizer"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imAf0WMuKZ0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af885cc-e60a-47e4-c64e-7c5f9c8fec1b"
      },
      "source": [
        "\n",
        "# Make TF-IDF\n",
        "vectorizer = CountVectorizer(inputCol='bigrams', outputCol='tf').fit(bigramDataFrame)\n",
        "\n",
        "train = vectorizer.transform(bigramDataFrame)\n",
        "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\").fit(train)\n",
        "train = idf.transform(train)\n",
        "\n",
        "\n",
        "# Compute normalized TF-IDF\n",
        "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"tfidf_norm\")\n",
        "train = normalizer.transform(train)\n",
        "\n",
        "train.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text| id|               words|             bigrams|                  tf|               tfidf|          tfidf_norm|\n",
            "+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|menyrbie philgaha...|  0|[menyrbie, philga...|[menyrbie philgah...|(13238,[3236,1288...|(13238,[3236,1288...|(13238,[3236,1288...|\n",
            "|advice talk neigh...|  1|[advice, talk, ne...|[advice talk, tal...|(13238,[1,722,194...|(13238,[1,722,194...|(13238,[1,722,194...|\n",
            "|coronavirus austr...|  2|[coronavirus, aus...|[coronavirus aust...|(13238,[80,103,13...|(13238,[80,103,13...|(13238,[80,103,13...|\n",
            "|food stock empty ...|  3|[food, stock, emp...|[food stock, stoc...|(13238,[29,34,44,...|(13238,[29,34,44,...|(13238,[29,34,44,...|\n",
            "|ready supermarket...|  4|[ready, supermark...|[ready supermarke...|(13238,[44,45,124...|(13238,[44,45,124...|(13238,[44,45,124...|\n",
            "|news region first...|  5|[news, region, fi...|[news region, reg...|(13238,[4,32,40,6...|(13238,[4,32,40,6...|(13238,[4,32,40,6...|\n",
            "|cashier grocery s...|  6|[cashier, grocery...|[cashier grocery,...|(13238,[0,1357,25...|(13238,[0,1357,25...|(13238,[0,1357,25...|\n",
            "|supermarket today...|  7|[supermarket, tod...|[supermarket toda...|(13238,[4,156,672...|(13238,[4,156,672...|(13238,[4,156,672...|\n",
            "|retail store clas...|  8|[retail, store, c...|[retail store, st...|(13238,[3,75,181,...|(13238,[3,75,181,...|(13238,[3,75,181,...|\n",
            "|corona prevention...|  9|[corona, preventi...|[corona preventio...|(13238,[1,922,164...|(13238,[1,922,164...|(13238,[1,922,164...|\n",
            "|month hasnt crowd...| 10|[month, hasnt, cr...|[month hasnt, has...|(13238,[464,1845,...|(13238,[464,1845,...|(13238,[464,1845,...|\n",
            "|situation increas...| 11|[situation, incre...|[situation increa...|(13238,[294,476,5...|(13238,[294,476,5...|(13238,[294,476,5...|\n",
            "|horningsea caring...| 12|[horningsea, cari...|[horningsea carin...|(13238,[1,89,601,...|(13238,[1,89,601,...|(13238,[1,89,601,...|\n",
            "|dont need stock f...| 13|[dont, need, stoc...|[dont need, need ...|(13238,[5,242,404...|(13238,[5,242,404...|(13238,[5,242,404...|\n",
            "|adara release res...| 14|[adara, release, ...|[adara release, r...|(13238,[828,1635,...|(13238,[828,1635,...|(13238,[828,1635,...|\n",
            "|line grocery stor...| 15|[line, grocery, s...|[line grocery, gr...|(13238,[0,183,395...|(13238,[0,183,395...|(13238,[0,183,395...|\n",
            "|                    | 16|                  []|                  []|       (13238,[],[])|       (13238,[],[])|       (13238,[],[])|\n",
            "|eyeonthearctic ru...| 17|[eyeonthearctic, ...|[eyeonthearctic r...|(13238,[1135,2722...|(13238,[1135,2722...|(13238,[1135,2722...|\n",
            "|amazon glitch sty...| 18|[amazon, glitch, ...|[amazon glitch, g...|(13238,[8,36,177,...|(13238,[8,36,177,...|(13238,[8,36,177,...|\n",
            "|arent struggling ...| 19|[arent, strugglin...|[arent struggling...|(13238,[7,399,535...|(13238,[7,399,535...|(13238,[7,399,535...|\n",
            "+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XTCr02eFWPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16cee552-8e12-4da6-ee16-89c6947cc10b"
      },
      "source": [
        "searchword = Row(bigrams = [['social distancing']])\n",
        "keyword = spark.createDataFrame(searchword).toDF('bigrams')\n",
        "keyword.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|            bigrams|\n",
            "+-------------------+\n",
            "|[social distancing]|\n",
            "+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhYcjaiAXsiT"
      },
      "source": [
        "from pyspark.ml import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekOOBlL5PHOI",
        "outputId": "7fc0fa25-3c5a-4f8d-d4dc-0b03ec2527af"
      },
      "source": [
        "\n",
        "pipleine = Pipeline(stages=[vectorizer,idf,normalizer])\n",
        "query = pipleine.fit(keyword).transform(keyword)\n",
        "query = query.selectExpr(\"tfidf_norm as tfidf_query\")\n",
        "query = train.crossJoin(query)\n",
        "query.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "|                text| id|               words|             bigrams|                  tf|               tfidf|          tfidf_norm|       tfidf_query|\n",
            "+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "|menyrbie philgaha...|  0|[menyrbie, philga...|[menyrbie philgah...|(13238,[3236,1288...|(13238,[3236,1288...|(13238,[3236,1288...|(13238,[12],[1.0])|\n",
            "|advice talk neigh...|  1|[advice, talk, ne...|[advice talk, tal...|(13238,[1,722,194...|(13238,[1,722,194...|(13238,[1,722,194...|(13238,[12],[1.0])|\n",
            "|coronavirus austr...|  2|[coronavirus, aus...|[coronavirus aust...|(13238,[80,103,13...|(13238,[80,103,13...|(13238,[80,103,13...|(13238,[12],[1.0])|\n",
            "|food stock empty ...|  3|[food, stock, emp...|[food stock, stoc...|(13238,[29,34,44,...|(13238,[29,34,44,...|(13238,[29,34,44,...|(13238,[12],[1.0])|\n",
            "|ready supermarket...|  4|[ready, supermark...|[ready supermarke...|(13238,[44,45,124...|(13238,[44,45,124...|(13238,[44,45,124...|(13238,[12],[1.0])|\n",
            "|news region first...|  5|[news, region, fi...|[news region, reg...|(13238,[4,32,40,6...|(13238,[4,32,40,6...|(13238,[4,32,40,6...|(13238,[12],[1.0])|\n",
            "|cashier grocery s...|  6|[cashier, grocery...|[cashier grocery,...|(13238,[0,1357,25...|(13238,[0,1357,25...|(13238,[0,1357,25...|(13238,[12],[1.0])|\n",
            "|supermarket today...|  7|[supermarket, tod...|[supermarket toda...|(13238,[4,156,672...|(13238,[4,156,672...|(13238,[4,156,672...|(13238,[12],[1.0])|\n",
            "|retail store clas...|  8|[retail, store, c...|[retail store, st...|(13238,[3,75,181,...|(13238,[3,75,181,...|(13238,[3,75,181,...|(13238,[12],[1.0])|\n",
            "|corona prevention...|  9|[corona, preventi...|[corona preventio...|(13238,[1,922,164...|(13238,[1,922,164...|(13238,[1,922,164...|(13238,[12],[1.0])|\n",
            "|month hasnt crowd...| 10|[month, hasnt, cr...|[month hasnt, has...|(13238,[464,1845,...|(13238,[464,1845,...|(13238,[464,1845,...|(13238,[12],[1.0])|\n",
            "|situation increas...| 11|[situation, incre...|[situation increa...|(13238,[294,476,5...|(13238,[294,476,5...|(13238,[294,476,5...|(13238,[12],[1.0])|\n",
            "|horningsea caring...| 12|[horningsea, cari...|[horningsea carin...|(13238,[1,89,601,...|(13238,[1,89,601,...|(13238,[1,89,601,...|(13238,[12],[1.0])|\n",
            "|dont need stock f...| 13|[dont, need, stoc...|[dont need, need ...|(13238,[5,242,404...|(13238,[5,242,404...|(13238,[5,242,404...|(13238,[12],[1.0])|\n",
            "|adara release res...| 14|[adara, release, ...|[adara release, r...|(13238,[828,1635,...|(13238,[828,1635,...|(13238,[828,1635,...|(13238,[12],[1.0])|\n",
            "|line grocery stor...| 15|[line, grocery, s...|[line grocery, gr...|(13238,[0,183,395...|(13238,[0,183,395...|(13238,[0,183,395...|(13238,[12],[1.0])|\n",
            "|                    | 16|                  []|                  []|       (13238,[],[])|       (13238,[],[])|       (13238,[],[])|(13238,[12],[1.0])|\n",
            "|eyeonthearctic ru...| 17|[eyeonthearctic, ...|[eyeonthearctic r...|(13238,[1135,2722...|(13238,[1135,2722...|(13238,[1135,2722...|(13238,[12],[1.0])|\n",
            "|amazon glitch sty...| 18|[amazon, glitch, ...|[amazon glitch, g...|(13238,[8,36,177,...|(13238,[8,36,177,...|(13238,[8,36,177,...|(13238,[12],[1.0])|\n",
            "|arent struggling ...| 19|[arent, strugglin...|[arent struggling...|(13238,[7,399,535...|(13238,[7,399,535...|(13238,[7,399,535...|(13238,[12],[1.0])|\n",
            "+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQFNg0_AYVa7"
      },
      "source": [
        "dot_udf = F.udf(lambda x,y: float(x.dot(y)), DoubleType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gkirojo5Xi0W",
        "outputId": "c233d6b8-8754-48a6-b0e3-c00a6603618b"
      },
      "source": [
        "query = query.withColumn('similariy', dot_udf(\"tfidf_query\", \"tfidf_norm\"))\n",
        "query= query.filter(\"similariy > 0\").orderBy('similariy', ascending=False).select(\"text\").rdd.flatMap(lambda x: x)\n",
        "query.take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['amid social distancing crisis starbucks move togo',\n",
              " 'social distancing prevent spread coronavirus could devastating effect people depression',\n",
              " 'basic protective measure maintain social distancing take care virtual online shopping example',\n",
              " 'social distancing starting little stir crazy practicing supermarket sweep pointer lesdoggg lockedupwithatoddler',\n",
              " 'government say start social distancing work retail cant talk customer store going catch',\n",
              " 'recession different service recession driven reduced consumer spending restaurant transportation personal care servicesnecessary social distancing writes dkuehn',\n",
              " 'herionlines fight infection believe social distancing avoid contact heri ensures dont worry shopping safest space shall deliver',\n",
              " 'politician need come retail store people practicing self quarantine social distancing even close elected make hard decision obviously people making',\n",
              " 'need part slowing spread practice social distancing result well closing retail store richmond march still shop online take care stay safe everyone',\n",
              " 'dear coronavirus following social distancing rule staying home prevent spread however spent alarming amount money shopping online submit expense reimbursement know coronapocolypse coronavirus']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVUBD7xhLquB",
        "outputId": "05c0ee63-0625-414d-a9e7-77a0a7fb4023"
      },
      "source": [
        "corpus.unpersist()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[98] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_esRMGklXkr8"
      },
      "source": [
        "from pyspark.ml.clustering import LDA"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z0cppvH_67T"
      },
      "source": [
        "number_topic=3\n",
        "max_number_iteration=20\n",
        "\n",
        "# Using lda model to train topic, assuming for 3 topic\n",
        "\n",
        "def print_topic(a,number_topic,tf_model):\n",
        "    '''\n",
        "    input: topics is the output of describeTopics()\n",
        "    '''\n",
        "\n",
        "    b=a.head(number_topic)\n",
        "    for i in range(number_topic):\n",
        "        topics1=[]\n",
        "        for j in b[i]['termIndices']:\n",
        "            topics1.append(tf_model.vocabulary[j])\n",
        "        print('The number {} topics: {}'.format(i+1,topics1))\n",
        "\n",
        "def lda_model(dataframe,number_topic,max_number_iteration,tf_model):\n",
        "\n",
        "    '''LDA model '''\n",
        "    ldf=LDA(k=number_topic,featuresCol='tf',maxIter=max_number_iteration,seed=1)\n",
        "    ldf_model=ldf.fit(dataframe)\n",
        "\n",
        "    topics=ldf_model.describeTopics()\n",
        "\n",
        "    a=topics.select('termIndices')\n",
        "    print_topic(a,number_topic,tf_model)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmVXalP_IU14",
        "outputId": "6d4457eb-953a-46d1-ac75-dd1613d21420"
      },
      "source": [
        "lda_model(train,number_topic,max_number_iteration,vectorizer)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number 1 topics: ['grocery store', 'panic buying', 'retail store', 'toilet paper', 'coronavirusoutbreak coronavirus', 'local grocery', 'store closure', 'coronavirus coronavirusoutbreak', 'store today', 'going grocery']\n",
            "The number 2 topics: ['food supply', 'food bank', 'online shopping', 'panic buying', 'grocery shopping', 'need panic', 'online grocery', 'supply chain', 'retail worker', 'really need']\n",
            "The number 3 topics: ['online shopping', 'grocery store', 'toilet paper', 'take care', 'hand sanitizer', 'stay home', 'social distancing', 'retail store', 'stock food', 'time crisis']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CgXZR6JLJKZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}