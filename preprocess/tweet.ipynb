{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "  try:\n",
    "      bom_removed = text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "  except:\n",
    "      bom_removed = text\n",
    "  stripped = re.sub(combined_pat, '', bom_removed)\n",
    "  stripped = re.sub(www_pat, '', stripped)\n",
    "  lower_case = stripped.lower()\n",
    "  neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "  letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "  # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "  # I will tokenize and join together to remove unneccessary white spaces\n",
    "  words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "  # words = [x for x  in tok.tokenize(neg_handled) if len(x) > 1]\n",
    "  return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "word_tokenizer = TweetTokenizer(strip_handles=True, preserve_case=False, reduce_len=True)\n",
    "def remove_punc_and_stopword(text):\n",
    "  punc_removed = [word for word in text if word not in string.punctuation]\n",
    "  punc_removed = ''.join(punc_removed)\n",
    "  punc_removed = word_tokenizer.tokenize(punc_removed)\n",
    "  vocabularies = []\n",
    "  for token in punc_removed :\n",
    "    if token.lower() not in sw:\n",
    "      vocabularies.append(token.lower())\n",
    "  return vocabularies\n",
    "\n",
    "def rem_single_characters_and_http(lst):\n",
    "  outputlst = []\n",
    "  for word in lst:\n",
    "    if word.startswith(\"http\") == False:\n",
    "      temp = re.sub('[^a-zA-Z ]+',' ', word) \n",
    "      temp=re.sub(\"&lt;/?.*?&gt;\",' ',temp)\n",
    "      temp=re.sub(\"(\\\\d|\\\\W)+\",\" \",temp)\n",
    "      if(len(temp)<=3):\n",
    "        outputlst.append(' ')        \n",
    "      else:\n",
    "        outputlst.append(temp)\n",
    "  return outputlst\n",
    "\n",
    "filter_words = ['covid']\n",
    "def lemmatizationFunct(x):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  finalLem = []\n",
    "  for s in x:\n",
    "    vab = lemmatizer.lemmatize(s)\n",
    "    if vab.startswith(\" \") == False and vab not in filter_words:\n",
    "      finalLem.append(vab)\n",
    "    # finalLem.append(vab)\n",
    "  return finalLem\n",
    "\n",
    "\n",
    "def joinTokensFunct(x):\n",
    "  x = \" \".join(x)\n",
    "  return x\n",
    "\n",
    "def removecharacters(text):   \n",
    "  text=text.strip()\n",
    "  text = re.sub('\\s+', ' ', text).strip()\n",
    "  return text\n",
    "\n",
    "\n",
    "def process(text):\n",
    "  text = tweet_cleaner(text)\n",
    "  text = remove_punc_and_stopword(text)\n",
    "  text = rem_single_characters_and_http(text)\n",
    "  text = lemmatizationFunct(text)\n",
    "  text = joinTokensFunct(text)\n",
    "  text = removecharacters(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sia = SIA()\n",
    "def get_sentiment_scores_neg(text):\n",
    "    pol_score = sia.polarity_scores(text)\n",
    "    return pol_score['neg']\n",
    "    \n",
    "def get_sentiment_scores_neu(text):\n",
    "    pol_score = sia.polarity_scores(text)\n",
    "    return pol_score['neu']\n",
    "    \n",
    "def get_sentiment_scores_pos(text):\n",
    "    pol_score = sia.polarity_scores(text)\n",
    "    return pol_score['pos']\n",
    "    \n",
    "def get_sentiment_scores_compound(text):\n",
    "    pol_score = sia.polarity_scores(text)\n",
    "    return pol_score['compound']\n",
    "    \n",
    "def get_sentiment_scores_label(text):\n",
    "    pol_score = sia.polarity_scores(text)\n",
    "    label = 0\n",
    "    if pol_score['compound'] > 0.2:\n",
    "        label = 1\n",
    "    elif pol_score['compound'] < -0.2:\n",
    "        label = -1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('/home/hkust/tweets.csv', chunksize=10000)\n",
    "count = 0\n",
    "for tweet in tweets:\n",
    "    tweet['text_cleaned'] = tweet.text.map(process)\n",
    "    tweet['neg'] = tweet.text.map(get_sentiment_scores_neg)\n",
    "    tweet['neu'] = tweet.text.map(get_sentiment_scores_neu)\n",
    "    tweet['pos'] = tweet.text.map(get_sentiment_scores_pos)\n",
    "    tweet['compound'] = tweet.text.map(get_sentiment_scores_compound)\n",
    "    tweet['label'] = tweet.text.map(get_sentiment_scores_label)\n",
    "    if count == 0:\n",
    "        tweet.to_csv(\"/home/hkust/tweets_cleaned.csv\", header=True, index=False)\n",
    "    else:\n",
    "        tweet.to_csv(\"/home/hkust/tweets_cleaned.csv\", mode='a', header=False, index=False)\n",
    "    print(count)\n",
    "    count += 1"
   ]
  }
 ]
}