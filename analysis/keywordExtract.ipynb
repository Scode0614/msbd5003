{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keywordExtract.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JedE2T3lGNJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e0d3f4-2f85-4098-88d0-40d207118d6d"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_275\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)\n",
            "Collecting pyspark==2.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 73kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 42.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130389 sha256=7eb783cdc3675eb5a0bfa321e4ab7ec9d4060ce2d9ba3b248229c5a4bf804eb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "Collecting spark-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/26/f7a6ac12339d2f1ed271c46c16705665620059e4559f323695925f3c63b4/spark_nlp-2.6.4-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 4.1MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.6.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4O0ukvcGMu1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "bc56cfda-f6fc-4902-ce0a-a0d793f4d89a"
      },
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version\")\n",
        "sparknlp.version()\n",
        "print(\"Apache Spark version\")\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version\n",
            "Apache Spark version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UpWXXfUGEWq"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import col, lit\n",
        "from functools import reduce\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from pyspark.ml.feature import CountVectorizer, MinHashLSH\n",
        "from pyspark.mllib.clustering import LDA, LDAModel\n",
        "from pyspark.mllib.linalg import Vector, Vectors\n",
        "# from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "conf=SparkConf().setAppName(\"miniProject\").setMaster(\"local[*]\")\n",
        "sc=SparkContext.getOrCreate(conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhv8MnibEZe5"
      },
      "source": [
        "# def insert2mongodb_2(df):\n",
        "#     mongoClient = MongoClient(\"mongodb://hkust:hkustAb$13gid@52.229.166.95:27017/\")\n",
        "#     db = mongoClient[\"MSBD5003\"]\n",
        "\n",
        "\n",
        "#     # connection = MongoClient(\"mongodb://msbd5003-db-server.eastasia.cloudapp.azure.com:27017/\")\n",
        "#     # db = connection['MSBD5003']\n",
        "#     # db.authenticate(\"hkust\", \"hkustAb$13gid\")\n",
        "#     All = []\n",
        "#     collection = db.tweets\n",
        "#     for index, row in df.iterrows():\n",
        "#         if(row['language'] == 'en'):\n",
        "#             tweet_dic = {'id': row['id'], 'text':row['tweet'],'created':row['date']}\n",
        "#             All.append(tweet_dic)\n",
        "#     #         tweet_dic = {'text':row['text'],'created':row['date']}\n",
        "#     collection.insert_many(All)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0jUwj-XHnsi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ccf762bf-466e-4618-e1f1-4fb0cd9069f9"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/archive-3/1.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserName</th>\n",
              "      <th>ScreenName</th>\n",
              "      <th>Location</th>\n",
              "      <th>TweetAt</th>\n",
              "      <th>OriginalTweet</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>44953</td>\n",
              "      <td>NYC</td>\n",
              "      <td>02-03-2020</td>\n",
              "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
              "      <td>Extremely Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>44954</td>\n",
              "      <td>Seattle, WA</td>\n",
              "      <td>02-03-2020</td>\n",
              "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>44955</td>\n",
              "      <td>NaN</td>\n",
              "      <td>02-03-2020</td>\n",
              "      <td>Find out how you can protect yourself and love...</td>\n",
              "      <td>Extremely Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>44956</td>\n",
              "      <td>Chicagoland</td>\n",
              "      <td>02-03-2020</td>\n",
              "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>44957</td>\n",
              "      <td>Melbourne, Victoria</td>\n",
              "      <td>03-03-2020</td>\n",
              "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   UserName  ...           Sentiment\n",
              "0         1  ...  Extremely Negative\n",
              "1         2  ...            Positive\n",
              "2         3  ...  Extremely Positive\n",
              "3         4  ...            Negative\n",
              "4         5  ...             Neutral\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS2NnbwiIsfc"
      },
      "source": [
        "\n",
        "\n",
        "pat1 = r'@[A-Za-z0-9_]+'\n",
        "pat2 = r'https?://[^ ]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "www_pat = r'www.[^ ]+'\n",
        "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\"}\n",
        "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "  try:\n",
        "      bom_removed = text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "  except:\n",
        "      bom_removed = text\n",
        "  stripped = re.sub(combined_pat, '', bom_removed)\n",
        "  stripped = re.sub(www_pat, '', stripped)\n",
        "  lower_case = stripped.lower()\n",
        "  neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
        "  letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
        "  # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
        "  # I will tokenize and join together to remove unneccessary white spaces\n",
        "\n",
        "  # words = [x for x  in tok.tokenize(neg_handled) if len(x) > 1]\n",
        "  return letters_only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An9kThWTH7j_"
      },
      "source": [
        "def removecharacters(text):   \n",
        "    text=text.strip()\n",
        "    text = re.sub('\\s+', ' ', text).strip()\n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAdz_sSDNEt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c98d745-a56a-48db-d153-943cc4041733"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "nltk.download('stopwords')\n",
        "sw = stopwords.words('english')\n",
        "word_tokenizer = TweetTokenizer(strip_handles=True, preserve_case=False, reduce_len=True)\n",
        "def remove_punc_and_stopword(text):\n",
        "    punc_removed = [word for word in text if word not in string.punctuation]\n",
        "    punc_removed = ''.join(punc_removed)\n",
        "    punc_removed = word_tokenizer.tokenize(punc_removed)\n",
        "    vocabularies = []\n",
        "    for token in punc_removed :\n",
        "        if token.lower() not in sw:\n",
        "            vocabularies.append(token.lower())\n",
        "    return vocabularies"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzHIv3NVN2e3"
      },
      "source": [
        "def rem_single_characters_and_http(lst):\n",
        "    outputlst = []\n",
        "    for word in lst:\n",
        "      if word.startswith(\"http\") == False:\n",
        "        temp = re.sub('[^a-zA-Z ]+',' ', word) \n",
        "        temp=re.sub(\"&lt;/?.*?&gt;\",' ',temp)\n",
        "        temp=re.sub(\"(\\\\d|\\\\W)+\",\" \",temp)\n",
        "        if(len(temp)<=3):\n",
        "            outputlst.append(' ')        \n",
        "        else:\n",
        "            outputlst.append(temp)\n",
        "    return outputlst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHPqagoKPsQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbaf0467-d47a-4b3c-920e-e14729571710"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "filter_words = ['covid']\n",
        "def lemmatizationFunct(x):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    finalLem = []\n",
        "    for s in x:\n",
        "      vab = lemmatizer.lemmatize(s)\n",
        "      if vab.startswith(\" \") == False and vab not in filter_words:\n",
        "        finalLem.append(vab)\n",
        "    return finalLem"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P38_cNNRQi_Q"
      },
      "source": [
        "def joinTokensFunct(x):\n",
        "    x = \" \".join(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF1YSPDLQyfi"
      },
      "source": [
        "def removecharacters(text):   \n",
        "    text=text.strip()\n",
        "    text = re.sub('\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wme8e5-5IK6Z"
      },
      "source": [
        "lines = df['OriginalTweet'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pjqABy8LExP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df850cf8-29d5-40ae-d0f4-c7d6d89148f5"
      },
      "source": [
        "RDD = sc.parallelize(lines)\n",
        "RDD.take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TRENDING: New Yorkers encounter empty supermarket shelves (pictured, Wegmans in Brooklyn), sold-out online grocers (FoodKick, MaxDelivery) as #coronavirus-fearing shoppers stock up https://t.co/Gr76pcrLWh https://t.co/ivMKMsqdT1',\n",
              " \"When I couldn't find hand sanitizer at Fred Meyer, I turned to #Amazon. But $114.97 for a 2 pack of Purell??!!Check out how  #coronavirus concerns are driving up prices. https://t.co/ygbipBflMY\",\n",
              " 'Find out how you can protect yourself and loved ones from #coronavirus. ?',\n",
              " '#Panic buying hits #NewYork City as anxious shoppers stock up on food&amp;medical supplies after #healthcare worker in her 30s becomes #BigApple 1st confirmed #coronavirus patient OR a #Bloomberg staged event?\\r\\r\\n\\r\\r\\nhttps://t.co/IASiReGPC4\\r\\r\\n\\r\\r\\n#QAnon #QAnon2018 #QAnon2020 \\r\\r\\n#Election2020 #CDC https://t.co/29isZOewxu',\n",
              " '#toiletpaper #dunnypaper #coronavirus #coronavirusaustralia #CoronaVirusUpdate #Covid_19 #9News  #Corvid19 #7NewsMelb #dunnypapergate #Costco    One week everyone buying baby milk powder the next everyone buying up toilet paper. https://t.co/ScZryVvsIh',\n",
              " 'Do you remember the last time you paid $2.99 a gallon for regular gas in Los Angeles?Prices at the pump are going down. A look at how the #coronavirus is impacting prices. 4pm @ABC7 https://t.co/Pyzq8YMuV5',\n",
              " 'Voting in the age of #coronavirus = hand sanitizer ? #SuperTuesday https://t.co/z0BeL4O6Dk',\n",
              " '@DrTedros \"We can\\x92t stop #COVID19 without protecting #healthworkers.\\r\\r\\nPrices of surgical masks have increased six-fold, N95 respirators have more than trebled &amp; gowns cost twice as much\"-@DrTedros #coronavirus',\n",
              " 'HI TWITTER! I am a pharmacist. I sell hand sanitizer for a living! Or I do when any exists. Like masks, it is sold the fuck out everywhere. SHOULD YOU BE WORRIED? No. Use soap. SHOULD YOU VISIT TWENTY PHARMACIES LOOKING FOR THE LAST BOTTLE? No. Pharmacies are full of sick people.',\n",
              " 'Anyone been in a supermarket over the last few days? Went to do my NORMAL shop last night &amp; ??is the sight that greeted me. Barmy! (Btw, what\\x92s so special about tinned tomatoes? ????????????). #Covid_19 #Dublin https://t.co/rGsM8xUxr6']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlaeWSEVJSid"
      },
      "source": [
        "removeNegRDD = RDD.map(tweet_cleaner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSMS1ogeLeMt"
      },
      "source": [
        "LC_vab_RDD = removeNegRDD.map(remove_punc_and_stopword)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiyB7HYXNbfa"
      },
      "source": [
        "rem_RDD = LC_vab_RDD.map(rem_single_characters_and_http)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdNdo-_oN_IE"
      },
      "source": [
        "lem_wordsRDD = rem_RDD.map(lemmatizationFunct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edLsLCRRPy18"
      },
      "source": [
        "joinedTokens = lem_wordsRDD.map(joinTokensFunct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdr9yfh5QgYW"
      },
      "source": [
        "rem_spacesRDD = joinedTokens.map(removecharacters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "CaFYjqPpkuI3",
        "outputId": "f0d64f40-090d-4b83-e30f-fca2cb91ec6b"
      },
      "source": [
        "df.drop(['UserName','ScreenName','Location','Sentiment'],axis=1,inplace=True)\n",
        "df['after_text'] = rem_spacesRDD.collect()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetAt</th>\n",
              "      <th>OriginalTweet</th>\n",
              "      <th>after_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>02-03-2020</td>\n",
              "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
              "      <td>trending yorkers encounter empty supermarket s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>02-03-2020</td>\n",
              "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
              "      <td>could find hand sanitizer fred meyer turned am...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>02-03-2020</td>\n",
              "      <td>Find out how you can protect yourself and love...</td>\n",
              "      <td>find protect loved one coronavirus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>02-03-2020</td>\n",
              "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
              "      <td>panic buying hit newyork city anxious shopper ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>03-03-2020</td>\n",
              "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
              "      <td>toiletpaper dunnypaper coronavirus coronavirus...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      TweetAt  ...                                         after_text\n",
              "0  02-03-2020  ...  trending yorkers encounter empty supermarket s...\n",
              "1  02-03-2020  ...  could find hand sanitizer fred meyer turned am...\n",
              "2  02-03-2020  ...                 find protect loved one coronavirus\n",
              "3  02-03-2020  ...  panic buying hit newyork city anxious shopper ...\n",
              "4  03-03-2020  ...  toiletpaper dunnypaper coronavirus coronavirus...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUgkQtNRS3qI"
      },
      "source": [
        "wordcounts = rem_spacesRDD.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\\\n",
        "             .sortBy(lambda x : x[1],False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhTS5xAtW0kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fccb8580-0697-41a7-a298-2a52b62f38f3"
      },
      "source": [
        "wordcounts.take(30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('coronavirus', 4955),\n",
              " ('store', 2795),\n",
              " ('grocery', 2215),\n",
              " ('food', 2214),\n",
              " ('supermarket', 1989),\n",
              " ('price', 1774),\n",
              " ('people', 1680),\n",
              " ('consumer', 1191),\n",
              " ('panic', 1172),\n",
              " ('shopping', 984),\n",
              " ('need', 939),\n",
              " ('online', 908),\n",
              " ('time', 829),\n",
              " ('buying', 707),\n",
              " ('stock', 677),\n",
              " ('worker', 673),\n",
              " ('like', 670),\n",
              " ('shelf', 628),\n",
              " ('home', 605),\n",
              " ('pandemic', 579),\n",
              " ('supply', 573),\n",
              " ('work', 554),\n",
              " ('going', 527),\n",
              " ('help', 508),\n",
              " ('please', 503),\n",
              " ('retail', 457),\n",
              " ('business', 449),\n",
              " ('demand', 442),\n",
              " ('week', 440),\n",
              " ('shop', 435)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct-QHKTTYs6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff64301-b580-4c97-b1bb-48e60bd78689"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "df = spark.createDataFrame(df)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+--------------------+\n",
            "|   TweetAt|       OriginalTweet|          after_text|\n",
            "+----------+--------------------+--------------------+\n",
            "|02-03-2020|TRENDING: New Yor...|trending yorkers ...|\n",
            "|02-03-2020|When I couldn't f...|could find hand s...|\n",
            "|02-03-2020|Find out how you ...|find protect love...|\n",
            "|02-03-2020|#Panic buying hit...|panic buying hit ...|\n",
            "|03-03-2020|#toiletpaper #dun...|toiletpaper dunny...|\n",
            "|03-03-2020|Do you remember t...|remember last tim...|\n",
            "|03-03-2020|Voting in the age...|voting coronaviru...|\n",
            "|03-03-2020|@DrTedros \"We can...|stop without prot...|\n",
            "|04-03-2020|HI TWITTER! I am ...|twitter pharmacis...|\n",
            "|04-03-2020|Anyone been in a ...|anyone supermarke...|\n",
            "|04-03-2020|Best quality couc...|best quality couc...|\n",
            "|04-03-2020|Beware of counter...|beware counterfei...|\n",
            "|04-03-2020|Panic food buying...|panic food buying...|\n",
            "|04-03-2020|#Covid_19 Went to...|went grocery stor...|\n",
            "|04-03-2020|While we were bus...|busy watching ele...|\n",
            "|04-03-2020|#AirSewa \r\r\n",
            "\r\r\n",
            "@f...|airsewa providing...|\n",
            "|05-03-2020|What Precautionar...|precautionary mea...|\n",
            "|05-03-2020|When youre stock...|stockpiling food ...|\n",
            "|05-03-2020|That's about a we...|week optimistic p...|\n",
            "|05-03-2020|Studies show the ...|study show corona...|\n",
            "+----------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QmJZG2zZIME"
      },
      "source": [
        "from pyspark.ml.feature import NGram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTueFF0jgA_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a03b1e3-e1a6-41c9-96b0-a01f2387c359"
      },
      "source": [
        "finaldf2 = df.withColumn(\"words\", F.split(df['after_text'], ' '))\n",
        "finaldf2.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+--------------------+--------------------+\n",
            "|   TweetAt|       OriginalTweet|          after_text|               words|\n",
            "+----------+--------------------+--------------------+--------------------+\n",
            "|02-03-2020|TRENDING: New Yor...|trending yorkers ...|[trending, yorker...|\n",
            "|02-03-2020|When I couldn't f...|could find hand s...|[could, find, han...|\n",
            "|02-03-2020|Find out how you ...|find protect love...|[find, protect, l...|\n",
            "|02-03-2020|#Panic buying hit...|panic buying hit ...|[panic, buying, h...|\n",
            "|03-03-2020|#toiletpaper #dun...|toiletpaper dunny...|[toiletpaper, dun...|\n",
            "|03-03-2020|Do you remember t...|remember last tim...|[remember, last, ...|\n",
            "|03-03-2020|Voting in the age...|voting coronaviru...|[voting, coronavi...|\n",
            "|03-03-2020|@DrTedros \"We can...|stop without prot...|[stop, without, p...|\n",
            "|04-03-2020|HI TWITTER! I am ...|twitter pharmacis...|[twitter, pharmac...|\n",
            "|04-03-2020|Anyone been in a ...|anyone supermarke...|[anyone, supermar...|\n",
            "|04-03-2020|Best quality couc...|best quality couc...|[best, quality, c...|\n",
            "|04-03-2020|Beware of counter...|beware counterfei...|[beware, counterf...|\n",
            "|04-03-2020|Panic food buying...|panic food buying...|[panic, food, buy...|\n",
            "|04-03-2020|#Covid_19 Went to...|went grocery stor...|[went, grocery, s...|\n",
            "|04-03-2020|While we were bus...|busy watching ele...|[busy, watching, ...|\n",
            "|04-03-2020|#AirSewa \r\r\n",
            "\r\r\n",
            "@f...|airsewa providing...|[airsewa, providi...|\n",
            "|05-03-2020|What Precautionar...|precautionary mea...|[precautionary, m...|\n",
            "|05-03-2020|When youre stock...|stockpiling food ...|[stockpiling, foo...|\n",
            "|05-03-2020|That's about a we...|week optimistic p...|[week, optimistic...|\n",
            "|05-03-2020|Studies show the ...|study show corona...|[study, show, cor...|\n",
            "+----------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHTL99_EhOGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c7ea9c-ebca-4e3c-a372-99d2cc2d9e74"
      },
      "source": [
        "bigram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
        "bigramDataFrame = bigram.transform(finaldf2)\n",
        "bigramDataFrame.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+--------------------+--------------------+--------------------+\n",
            "|   TweetAt|       OriginalTweet|          after_text|               words|             bigrams|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+\n",
            "|02-03-2020|TRENDING: New Yor...|trending yorkers ...|[trending, yorker...|[trending yorkers...|\n",
            "|02-03-2020|When I couldn't f...|could find hand s...|[could, find, han...|[could find, find...|\n",
            "|02-03-2020|Find out how you ...|find protect love...|[find, protect, l...|[find protect, pr...|\n",
            "|02-03-2020|#Panic buying hit...|panic buying hit ...|[panic, buying, h...|[panic buying, bu...|\n",
            "|03-03-2020|#toiletpaper #dun...|toiletpaper dunny...|[toiletpaper, dun...|[toiletpaper dunn...|\n",
            "|03-03-2020|Do you remember t...|remember last tim...|[remember, last, ...|[remember last, l...|\n",
            "|03-03-2020|Voting in the age...|voting coronaviru...|[voting, coronavi...|[voting coronavir...|\n",
            "|03-03-2020|@DrTedros \"We can...|stop without prot...|[stop, without, p...|[stop without, wi...|\n",
            "|04-03-2020|HI TWITTER! I am ...|twitter pharmacis...|[twitter, pharmac...|[twitter pharmaci...|\n",
            "|04-03-2020|Anyone been in a ...|anyone supermarke...|[anyone, supermar...|[anyone supermark...|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h31VAzjhb7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0385bb5e-2eeb-4361-ab6d-708b3b130c1c"
      },
      "source": [
        "topbigrams = bigramDataFrame.withColumn('hot_2_gram', F.explode(F.col('bigrams')))\\\n",
        "    .groupBy('hot_2_gram')\\\n",
        "    .count()\\\n",
        "    .sort('count', ascending=False)\\\n",
        "    .select('hot_2_gram')\n",
        "topbigrams.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|          hot_2_gram|\n",
            "+--------------------+\n",
            "|       grocery store|\n",
            "|        panic buying|\n",
            "|     online shopping|\n",
            "|        toilet paper|\n",
            "|   supermarket shelf|\n",
            "|        store worker|\n",
            "|        retail store|\n",
            "|   social distancing|\n",
            "|coronavirus pandemic|\n",
            "|        supply chain|\n",
            "+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9bLKW5nNpUb",
        "outputId": "5311bb02-a4e8-44fa-bc3c-8a8e14a1a241"
      },
      "source": [
        "top_1_grams = bigramDataFrame.withColumn('hot_1_gram', F.explode(F.col('words')))\\\n",
        "    .groupBy('hot_1_gram')\\\n",
        "    .count()\\\n",
        "    .sort('count', ascending=False)\\\n",
        "    .select('hot_1_gram')\n",
        "top_1_grams.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "| hot_1_gram|\n",
            "+-----------+\n",
            "|coronavirus|\n",
            "|      store|\n",
            "|    grocery|\n",
            "|       food|\n",
            "|supermarket|\n",
            "|      price|\n",
            "|     people|\n",
            "|   consumer|\n",
            "|      panic|\n",
            "|   shopping|\n",
            "+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYlbSTkEhuA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd922b5-3e45-4c1f-a18d-301877d09f70"
      },
      "source": [
        "trigram = NGram(n=3, inputCol=\"words\", outputCol=\"trigrams\")\n",
        "trigramDataFrame = trigram.transform(bigramDataFrame)\n",
        "trigramDataFrame.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|   TweetAt|       OriginalTweet|          after_text|               words|             bigrams|            trigrams|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|02-03-2020|TRENDING: New Yor...|trending yorkers ...|[trending, yorker...|[trending yorkers...|[trending yorkers...|\n",
            "|02-03-2020|When I couldn't f...|could find hand s...|[could, find, han...|[could find, find...|[could find hand,...|\n",
            "|02-03-2020|Find out how you ...|find protect love...|[find, protect, l...|[find protect, pr...|[find protect lov...|\n",
            "|02-03-2020|#Panic buying hit...|panic buying hit ...|[panic, buying, h...|[panic buying, bu...|[panic buying hit...|\n",
            "|03-03-2020|#toiletpaper #dun...|toiletpaper dunny...|[toiletpaper, dun...|[toiletpaper dunn...|[toiletpaper dunn...|\n",
            "|03-03-2020|Do you remember t...|remember last tim...|[remember, last, ...|[remember last, l...|[remember last ti...|\n",
            "|03-03-2020|Voting in the age...|voting coronaviru...|[voting, coronavi...|[voting coronavir...|[voting coronavir...|\n",
            "|03-03-2020|@DrTedros \"We can...|stop without prot...|[stop, without, p...|[stop without, wi...|[stop without pro...|\n",
            "|04-03-2020|HI TWITTER! I am ...|twitter pharmacis...|[twitter, pharmac...|[twitter pharmaci...|[twitter pharmaci...|\n",
            "|04-03-2020|Anyone been in a ...|anyone supermarke...|[anyone, supermar...|[anyone supermark...|[anyone supermark...|\n",
            "|04-03-2020|Best quality couc...|best quality couc...|[best, quality, c...|[best quality, qu...|[best quality cou...|\n",
            "|04-03-2020|Beware of counter...|beware counterfei...|[beware, counterf...|[beware counterfe...|[beware counterfe...|\n",
            "|04-03-2020|Panic food buying...|panic food buying...|[panic, food, buy...|[panic food, food...|[panic food buyin...|\n",
            "|04-03-2020|#Covid_19 Went to...|went grocery stor...|[went, grocery, s...|[went grocery, gr...|[went grocery sto...|\n",
            "|04-03-2020|While we were bus...|busy watching ele...|[busy, watching, ...|[busy watching, w...|[busy watching el...|\n",
            "|04-03-2020|#AirSewa \r\r\n",
            "\r\r\n",
            "@f...|airsewa providing...|[airsewa, providi...|[airsewa providin...|[airsewa providin...|\n",
            "|05-03-2020|What Precautionar...|precautionary mea...|[precautionary, m...|[precautionary me...|[precautionary me...|\n",
            "|05-03-2020|When youre stock...|stockpiling food ...|[stockpiling, foo...|[stockpiling food...|[stockpiling food...|\n",
            "|05-03-2020|That's about a we...|week optimistic p...|[week, optimistic...|[week optimistic,...|[week optimistic ...|\n",
            "|05-03-2020|Studies show the ...|study show corona...|[study, show, cor...|[study show, show...|[study show coron...|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLF3OAO3aZPu"
      },
      "source": [
        "append_udf = F.udf(lambda x,y: x + y, ArrayType(StringType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG9MD7VhZl6F",
        "outputId": "81116396-a38e-4d53-c856-77472f9de204"
      },
      "source": [
        "res = trigramDataFrame.withColumn('features', F.concat(F.col('bigrams'),F.col('trigrams')))\n",
        "res.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|   TweetAt|       OriginalTweet|          after_text|               words|             bigrams|            trigrams|            features|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|02-03-2020|TRENDING: New Yor...|trending yorkers ...|[trending, yorker...|[trending yorkers...|[trending yorkers...|[trending yorkers...|\n",
            "|02-03-2020|When I couldn't f...|could find hand s...|[could, find, han...|[could find, find...|[could find hand,...|[could find, find...|\n",
            "|02-03-2020|Find out how you ...|find protect love...|[find, protect, l...|[find protect, pr...|[find protect lov...|[find protect, pr...|\n",
            "|02-03-2020|#Panic buying hit...|panic buying hit ...|[panic, buying, h...|[panic buying, bu...|[panic buying hit...|[panic buying, bu...|\n",
            "|03-03-2020|#toiletpaper #dun...|toiletpaper dunny...|[toiletpaper, dun...|[toiletpaper dunn...|[toiletpaper dunn...|[toiletpaper dunn...|\n",
            "|03-03-2020|Do you remember t...|remember last tim...|[remember, last, ...|[remember last, l...|[remember last ti...|[remember last, l...|\n",
            "|03-03-2020|Voting in the age...|voting coronaviru...|[voting, coronavi...|[voting coronavir...|[voting coronavir...|[voting coronavir...|\n",
            "|03-03-2020|@DrTedros \"We can...|stop without prot...|[stop, without, p...|[stop without, wi...|[stop without pro...|[stop without, wi...|\n",
            "|04-03-2020|HI TWITTER! I am ...|twitter pharmacis...|[twitter, pharmac...|[twitter pharmaci...|[twitter pharmaci...|[twitter pharmaci...|\n",
            "|04-03-2020|Anyone been in a ...|anyone supermarke...|[anyone, supermar...|[anyone supermark...|[anyone supermark...|[anyone supermark...|\n",
            "|04-03-2020|Best quality couc...|best quality couc...|[best, quality, c...|[best quality, qu...|[best quality cou...|[best quality, qu...|\n",
            "|04-03-2020|Beware of counter...|beware counterfei...|[beware, counterf...|[beware counterfe...|[beware counterfe...|[beware counterfe...|\n",
            "|04-03-2020|Panic food buying...|panic food buying...|[panic, food, buy...|[panic food, food...|[panic food buyin...|[panic food, food...|\n",
            "|04-03-2020|#Covid_19 Went to...|went grocery stor...|[went, grocery, s...|[went grocery, gr...|[went grocery sto...|[went grocery, gr...|\n",
            "|04-03-2020|While we were bus...|busy watching ele...|[busy, watching, ...|[busy watching, w...|[busy watching el...|[busy watching, w...|\n",
            "|04-03-2020|#AirSewa \r\r\n",
            "\r\r\n",
            "@f...|airsewa providing...|[airsewa, providi...|[airsewa providin...|[airsewa providin...|[airsewa providin...|\n",
            "|05-03-2020|What Precautionar...|precautionary mea...|[precautionary, m...|[precautionary me...|[precautionary me...|[precautionary me...|\n",
            "|05-03-2020|When youre stock...|stockpiling food ...|[stockpiling, foo...|[stockpiling food...|[stockpiling food...|[stockpiling food...|\n",
            "|05-03-2020|That's about a we...|week optimistic p...|[week, optimistic...|[week optimistic,...|[week optimistic ...|[week optimistic,...|\n",
            "|05-03-2020|Studies show the ...|study show corona...|[study, show, cor...|[study show, show...|[study show coron...|[study show, show...|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArO5dGjlJ6-i"
      },
      "source": [
        "topwords = trigramDataFrame.withColumn('hot_3_gram', F.explode(F.col('trigrams')))\\\n",
        "    .groupBy('hot_3_gram')\\\n",
        "    .count()\\\n",
        "    .sort('count', ascending=False)\\\n",
        "    .select('hot_3_gram')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z5dKVcaY8t9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paSzNwjXKCOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48cbb034-f83f-454f-d8be-44de3a64b319"
      },
      "source": [
        "topwords.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|          hot_3_gram|\n",
            "+--------------------+\n",
            "|grocery store worker|\n",
            "|grocery store emp...|\n",
            "|   stop panic buying|\n",
            "| local grocery store|\n",
            "|  went grocery store|\n",
            "|  work grocery store|\n",
            "| going grocery store|\n",
            "|   panic buying food|\n",
            "|grocery store cor...|\n",
            "| grocery store shelf|\n",
            "+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUQzSpbbKIQ5"
      },
      "source": [
        "# topwords.write.csv('topwords_trigrams1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_gnxzscKR7x"
      },
      "source": [
        "# list_trigrams = topwords.select(\"word\").rdd.flatMap(lambda x: x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7hSOg5PKVgn"
      },
      "source": [
        "# list_trigrams.take(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyEdilGNFibU"
      },
      "source": [
        "from pyspark.ml.feature import Normalizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imAf0WMuKZ0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9741a66f-7b44-40b3-e4d8-f4084be9dd27"
      },
      "source": [
        "\n",
        "# Make TF-IDF\n",
        "vectorizer = CountVectorizer(inputCol='features', outputCol='tf').fit(res)\n",
        "\n",
        "train = vectorizer.transform(res)\n",
        "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\").fit(train)\n",
        "train = idf.transform(train)\n",
        "\n",
        "\n",
        "# Compute normalized TF-IDF\n",
        "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"tfidf_norm\")\n",
        "train = normalizer.transform(train)\n",
        "\n",
        "train.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|   TweetAt|       OriginalTweet|          after_text|               words|             bigrams|            trigrams|            features|                  tf|               tfidf|          tfidf_norm|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|02-03-2020|TRENDING: New Yor...|trending yorkers ...|[trending, yorker...|[trending yorkers...|[trending yorkers...|[trending yorkers...|(92859,[19,304,79...|(92859,[19,304,79...|(92859,[19,304,79...|\n",
            "|02-03-2020|When I couldn't f...|could find hand s...|[could, find, han...|[could find, find...|[could find hand,...|[could find, find...|(92859,[8,1064,27...|(92859,[8,1064,27...|(92859,[8,1064,27...|\n",
            "|02-03-2020|Find out how you ...|find protect love...|[find, protect, l...|[find protect, pr...|[find protect lov...|[find protect, pr...|(92859,[1228,5996...|(92859,[1228,5996...|(92859,[1228,5996...|\n",
            "|02-03-2020|#Panic buying hit...|panic buying hit ...|[panic, buying, h...|[panic buying, bu...|[panic buying hit...|[panic buying, bu...|(92859,[2,4,361,7...|(92859,[2,4,361,7...|(92859,[2,4,361,7...|\n",
            "|03-03-2020|#toiletpaper #dun...|toiletpaper dunny...|[toiletpaper, dun...|[toiletpaper dunn...|[toiletpaper dunn...|[toiletpaper dunn...|(92859,[1,59,75,7...|(92859,[1,59,75,7...|(92859,[1,59,75,7...|\n",
            "|03-03-2020|Do you remember t...|remember last tim...|[remember, last, ...|[remember last, l...|[remember last ti...|[remember last, l...|(92859,[759,1686,...|(92859,[759,1686,...|(92859,[759,1686,...|\n",
            "|03-03-2020|Voting in the age...|voting coronaviru...|[voting, coronavi...|[voting coronavir...|[voting coronavir...|[voting coronavir...|(92859,[8,12469,2...|(92859,[8,12469,2...|(92859,[8,12469,2...|\n",
            "|03-03-2020|@DrTedros \"We can...|stop without prot...|[stop, without, p...|[stop without, wi...|[stop without pro...|[stop without, wi...|(92859,[466,1351,...|(92859,[466,1351,...|(92859,[466,1351,...|\n",
            "|04-03-2020|HI TWITTER! I am ...|twitter pharmacis...|[twitter, pharmac...|[twitter pharmaci...|[twitter pharmaci...|[twitter pharmaci...|(92859,[8,2121,34...|(92859,[8,2121,34...|(92859,[8,2121,34...|\n",
            "|04-03-2020|Anyone been in a ...|anyone supermarke...|[anyone, supermar...|[anyone supermark...|[anyone supermark...|[anyone supermark...|(92859,[61,679,76...|(92859,[61,679,76...|(92859,[61,679,76...|\n",
            "|04-03-2020|Best quality couc...|best quality couc...|[best, quality, c...|[best quality, qu...|[best quality cou...|[best quality, qu...|(92859,[8260,8453...|(92859,[8260,8453...|(92859,[8260,8453...|\n",
            "|04-03-2020|Beware of counter...|beware counterfei...|[beware, counterf...|[beware counterfe...|[beware counterfe...|[beware counterfe...|(92859,[2260,2833...|(92859,[2260,2833...|(92859,[2260,2833...|\n",
            "|04-03-2020|Panic food buying...|panic food buying...|[panic, food, buy...|[panic food, food...|[panic food buyin...|[panic food, food...|(92859,[4,139,113...|(92859,[4,139,113...|(92859,[4,139,113...|\n",
            "|04-03-2020|#Covid_19 Went to...|went grocery stor...|[went, grocery, s...|[went grocery, gr...|[went grocery sto...|[went grocery, gr...|(92859,[0,8,17,20...|(92859,[0,8,17,20...|(92859,[0,8,17,20...|\n",
            "|04-03-2020|While we were bus...|busy watching ele...|[busy, watching, ...|[busy watching, w...|[busy watching el...|[busy watching, w...|(92859,[1365,9458...|(92859,[1365,9458...|(92859,[1365,9458...|\n",
            "|04-03-2020|#AirSewa \r\r\n",
            "\r\r\n",
            "@f...|airsewa providing...|[airsewa, providi...|[airsewa providin...|[airsewa providin...|[airsewa providin...|(92859,[2636,3743...|(92859,[2636,3743...|(92859,[2636,3743...|\n",
            "|05-03-2020|What Precautionar...|precautionary mea...|[precautionary, m...|[precautionary me...|[precautionary me...|[precautionary me...|(92859,[1282,2315...|(92859,[1282,2315...|(92859,[1282,2315...|\n",
            "|05-03-2020|When youre stock...|stockpiling food ...|[stockpiling, foo...|[stockpiling food...|[stockpiling food...|[stockpiling food...|(92859,[6,7,32,47...|(92859,[6,7,32,47...|(92859,[6,7,32,47...|\n",
            "|05-03-2020|That's about a we...|week optimistic p...|[week, optimistic...|[week optimistic,...|[week optimistic ...|[week optimistic,...|(92859,[38,291,69...|(92859,[38,291,69...|(92859,[38,291,69...|\n",
            "|05-03-2020|Studies show the ...|study show corona...|[study, show, cor...|[study show, show...|[study show coron...|[study show, show...|(92859,[1246,4332...|(92859,[1246,4332...|(92859,[1246,4332...|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XTCr02eFWPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2386e19f-129f-4215-b11a-4be0e1d8452f"
      },
      "source": [
        "searchword = Row(bigrams = [['social distancing']])\n",
        "keyword = spark.createDataFrame(searchword).toDF('features')\n",
        "keyword.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|           features|\n",
            "+-------------------+\n",
            "|[social distancing]|\n",
            "+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhYcjaiAXsiT"
      },
      "source": [
        "from pyspark.ml import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekOOBlL5PHOI",
        "outputId": "66b9eedc-90b9-4715-f05f-125d1518c8de"
      },
      "source": [
        "\n",
        "pipleine = Pipeline(stages=[vectorizer,idf,normalizer])\n",
        "query = pipleine.fit(keyword).transform(keyword)\n",
        "query = query.selectExpr(\"tfidf_norm as tfidf_query\")\n",
        "query = train.crossJoin(query)\n",
        "query.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "|   TweetAt|       OriginalTweet|          after_text|               words|             bigrams|                  tf|               tfidf|          tfidf_norm|       tfidf_query|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "|16-03-2020|@MeNyrbie @Phil_G...|                    |                  []|                  []|      (102343,[],[])|      (102343,[],[])|      (102343,[],[])|(102343,[7],[1.0])|\n",
            "|16-03-2020|advice Talk to yo...|advice talk neigh...|[advice, talk, ne...|[advice talk, tal...|(102343,[2,3871,8...|(102343,[2,3871,8...|(102343,[2,3871,8...|(102343,[7],[1.0])|\n",
            "|16-03-2020|Coronavirus Austr...|coronavirus austr...|[coronavirus, aus...|[coronavirus aust...|(102343,[122,128,...|(102343,[122,128,...|(102343,[122,128,...|(102343,[7],[1.0])|\n",
            "|16-03-2020|My food stock is ...|food stock empty ...|[food, stock, emp...|[food stock, stoc...|(102343,[21,48,81...|(102343,[21,48,81...|(102343,[21,48,81...|(102343,[7],[1.0])|\n",
            "|16-03-2020|Me, ready to go a...|ready supermarket...|[ready, supermark...|[ready supermarke...|(102343,[81,586,1...|(102343,[81,586,1...|(102343,[81,586,1...|(102343,[7],[1.0])|\n",
            "|16-03-2020|As news of the re...|news region first...|[news, region, fi...|[news region, reg...|(102343,[3,16,84,...|(102343,[3,16,84,...|(102343,[3,16,84,...|(102343,[7],[1.0])|\n",
            "|16-03-2020|Cashier at grocer...|cashier grocery s...|[cashier, grocery...|[cashier grocery,...|(102343,[0,1296,2...|(102343,[0,1296,2...|(102343,[0,1296,2...|(102343,[7],[1.0])|\n",
            "|16-03-2020|Was at the superm...|supermarket today...|[supermarket, tod...|[supermarket toda...|(102343,[3,102,29...|(102343,[3,102,29...|(102343,[3,102,29...|(102343,[7],[1.0])|\n",
            "|16-03-2020|Due to COVID-19 o...|retail store clas...|[retail, store, c...|[retail store, st...|(102343,[6,105,16...|(102343,[6,105,16...|(102343,[6,105,16...|(102343,[7],[1.0])|\n",
            "|16-03-2020|For corona preven...|corona prevention...|[corona, preventi...|[corona preventio...|(102343,[2,918,43...|(102343,[2,918,43...|(102343,[2,918,43...|(102343,[7],[1.0])|\n",
            "|16-03-2020|All month there h...|month crowding su...|[month, crowding,...|[month crowding, ...|(102343,[2392,402...|(102343,[2392,402...|(102343,[2392,402...|(102343,[7],[1.0])|\n",
            "|16-03-2020|Due to the Covid-...|situation increas...|[situation, incre...|[situation increa...|(102343,[83,155,5...|(102343,[83,155,5...|(102343,[83,155,5...|(102343,[7],[1.0])|\n",
            "|16-03-2020|#horningsea is a ...|horningsea caring...|[horningsea, cari...|[horningsea carin...|(102343,[2,77,209...|(102343,[2,77,209...|(102343,[2,77,209...|(102343,[7],[1.0])|\n",
            "|16-03-2020|Me: I don't need ...|need stock food a...|[need, stock, foo...|[need stock, stoc...|(102343,[17,323,3...|(102343,[17,323,3...|(102343,[17,323,3...|(102343,[7],[1.0])|\n",
            "|16-03-2020|ADARA Releases CO...|adara release res...|[adara, release, ...|[adara release, r...|(102343,[1876,230...|(102343,[1876,230...|(102343,[1876,230...|(102343,[7],[1.0])|\n",
            "|16-03-2020|Lines at the groc...|line grocery stor...|[line, grocery, s...|[line grocery, gr...|(102343,[0,135,36...|(102343,[0,135,36...|(102343,[0,135,36...|(102343,[7],[1.0])|\n",
            "|16-03-2020|????? ????? ?????...|                    |                  []|                  []|      (102343,[],[])|      (102343,[],[])|      (102343,[],[])|(102343,[7],[1.0])|\n",
            "|16-03-2020|@eyeonthearctic 1...|russia consumer s...|[russia, consumer...|[russia consumer,...|(102343,[1946,380...|(102343,[1946,380...|(102343,[1946,380...|(102343,[7],[1.0])|\n",
            "|16-03-2020|Amazon Glitch Sty...|amazon glitch sty...|[amazon, glitch, ...|[amazon glitch, g...|(102343,[14,73,32...|(102343,[14,73,32...|(102343,[14,73,32...|(102343,[7],[1.0])|\n",
            "|16-03-2020|For those who are...|struggling please...|[struggling, plea...|[struggling pleas...|(102343,[13,431,5...|(102343,[13,431,5...|(102343,[13,431,5...|(102343,[7],[1.0])|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQFNg0_AYVa7"
      },
      "source": [
        "dot_udf = F.udf(lambda x,y: float(x.dot(y)), DoubleType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gkirojo5Xi0W",
        "outputId": "0ca8be7b-7903-454e-8c49-392ed0166ddc"
      },
      "source": [
        "query = query.withColumn('similariy', dot_udf(\"tfidf_query\", \"tfidf_norm\"))\n",
        "query= query.filter(\"similariy > 0\").orderBy('similariy', ascending=False).select(\"OriginalTweet\").rdd.flatMap(lambda x: x)\n",
        "query.take(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#COVID19PH #COVID2019 How can you maintain social distancing in a supermarket https://t.co/Zi2vjHRQT3',\n",
              " '@zuffle Ok.. maybe not stay at home.. but social distancing, in my book, is not going to a huge supermarket.. \\r\\r\\nAnd there definitely IS instruction for people over 70 to exercise social distancing..\\r\\r\\n\\r\\r\\nhttps://t.co/PObPOW1Sqn',\n",
              " \"Amid 'social distancing' during COVID-19 crisis, Starbucks moves to 'to-go' only https://t.co/HbJDKnjPrD\",\n",
              " 'Social Distancing amp Online Shopping happens to be things that I m good at  ',\n",
              " 'The @psucreamery has closed for retail sales due to the #coronavirus social distancing measures. https://t.co/l38rhBRpB2',\n",
              " 'Social distancing measures at a local grocery store on the Oregon coast They GET IT 19',\n",
              " 'Social distancing in a Danish Supermarket.\\r\\r\\nThe things this #covid19 #coronavirus is making us do, enh? https://t.co/HjPKTQ5reE',\n",
              " 'Having friends over for dinner/coffee is not social distancing.Visiting family in long-term care homes or hospital is not.Stopping at a grocery store to stock up after travel, including travel to the United States is not social distancing. https://t.co/7SLelpyZ3j',\n",
              " 'Have you been practicing social distancing? Not easy when you\\x92re a hugger. We found people close together at basketball court, grocery store. Health officials say it\\x92s important to buy time, the more people who practice social distancing, less people affected. #coronavirus #ktvu https://t.co/WUj5Mm3RTY',\n",
              " 'With U.S. consumers social distancing in response to the #coronavirus, online #grocery shopping has accelerated in an unprecedented way. https://t.co/cIpujpRa4s @JDMeltonDC360']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "oVUBD7xhLquB",
        "outputId": "db9827a5-62f9-4b39-eba8-4b6cea7f8ea4"
      },
      "source": [
        "corpus.unpersist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-4925618b5ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_esRMGklXkr8"
      },
      "source": [
        "from pyspark.ml.clustering import LDA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z0cppvH_67T"
      },
      "source": [
        "number_topic=5\n",
        "max_number_iteration=20\n",
        "\n",
        "# Using lda model to train topic, assuming for 3 topic\n",
        "\n",
        "def print_topic(a,number_topic,tf_model):\n",
        "    '''\n",
        "    input: topics is the output of describeTopics()\n",
        "    '''\n",
        "    b=a.head(number_topic)\n",
        "    for i in range(number_topic):\n",
        "        topics1=[]\n",
        "        k = 0\n",
        "        for j in b[i]['termIndices']:\n",
        "            # if k == 0:\n",
        "            #   k += 1\n",
        "              # continue\n",
        "            if k > 5:\n",
        "              break\n",
        "            k += 1\n",
        "            topics1.append(tf_model.vocabulary[j])\n",
        "        print('topic {} : {}'.format(i+1,topics1))\n",
        "\n",
        "def lda_model(dataframe,number_topic,max_number_iteration,tf_model):\n",
        "\n",
        "    '''LDA model '''\n",
        "    ldf=LDA(k=number_topic,featuresCol='tf',maxIter=max_number_iteration,seed=1)\n",
        "    ldf_model=ldf.fit(dataframe)\n",
        "\n",
        "    topics=ldf_model.describeTopics()\n",
        "\n",
        "    a=topics.select('termIndices')\n",
        "    print_topic(a,number_topic,tf_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmVXalP_IU14",
        "outputId": "a05f37fd-2feb-4a45-b1ce-64ad37171a3e"
      },
      "source": [
        "lda_model(train,number_topic,max_number_iteration,vectorizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "topic 1 : ['grocery store', 'toilet paper', 'panic buying', 'store shelf', 'paid sick', 'sick leave']\n",
            "topic 2 : ['online shopping', 'grocery store', 'stock food', 'food supply', 'food water', 'toilet paper']\n",
            "topic 3 : ['retail store', 'shopping online', 'food bank', 'hand sanitizer', 'grocery shopping', 'online grocery']\n",
            "topic 4 : ['stock food', 'online shopping', 'stay home', 'supermarket shelf', 'toilet paper', 'amid coronavirus']\n",
            "topic 5 : ['grocery store', 'panic buying', 'local grocery', 'local grocery store', 'empty shelf', 'food bank']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxyfbwrnvbsf"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkLHL3cuvdEK",
        "outputId": "99207dec-8181-45ac-de9d-877a9f679ae2"
      },
      "source": [
        "kmeans = KMeans(featuresCol='tf').setK(5).setMaxIter(20)\n",
        "km_model = kmeans.fit(train)\n",
        "clustersTable = km_model.transform(train)\n",
        "clustersTable.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|   TweetAt|       OriginalTweet|          after_text|               words|             bigrams|            trigrams|            features|                  tf|               tfidf|          tfidf_norm|prediction|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|02-03-2020|TRENDING: New Yor...|trending yorkers ...|[trending, yorker...|[trending yorkers...|[trending yorkers...|[trending yorkers...|(92859,[19,304,79...|(92859,[19,304,79...|(92859,[19,304,79...|         0|\n",
            "|02-03-2020|When I couldn't f...|could find hand s...|[could, find, han...|[could find, find...|[could find hand,...|[could find, find...|(92859,[8,1064,27...|(92859,[8,1064,27...|(92859,[8,1064,27...|         0|\n",
            "|02-03-2020|Find out how you ...|find protect love...|[find, protect, l...|[find protect, pr...|[find protect lov...|[find protect, pr...|(92859,[1228,5996...|(92859,[1228,5996...|(92859,[1228,5996...|         0|\n",
            "|02-03-2020|#Panic buying hit...|panic buying hit ...|[panic, buying, h...|[panic buying, bu...|[panic buying hit...|[panic buying, bu...|(92859,[2,4,361,7...|(92859,[2,4,361,7...|(92859,[2,4,361,7...|         0|\n",
            "|03-03-2020|#toiletpaper #dun...|toiletpaper dunny...|[toiletpaper, dun...|[toiletpaper dunn...|[toiletpaper dunn...|[toiletpaper dunn...|(92859,[1,59,75,7...|(92859,[1,59,75,7...|(92859,[1,59,75,7...|         0|\n",
            "|03-03-2020|Do you remember t...|remember last tim...|[remember, last, ...|[remember last, l...|[remember last ti...|[remember last, l...|(92859,[759,1686,...|(92859,[759,1686,...|(92859,[759,1686,...|         0|\n",
            "|03-03-2020|Voting in the age...|voting coronaviru...|[voting, coronavi...|[voting coronavir...|[voting coronavir...|[voting coronavir...|(92859,[8,12469,2...|(92859,[8,12469,2...|(92859,[8,12469,2...|         0|\n",
            "|03-03-2020|@DrTedros \"We can...|stop without prot...|[stop, without, p...|[stop without, wi...|[stop without pro...|[stop without, wi...|(92859,[466,1351,...|(92859,[466,1351,...|(92859,[466,1351,...|         0|\n",
            "|04-03-2020|HI TWITTER! I am ...|twitter pharmacis...|[twitter, pharmac...|[twitter pharmaci...|[twitter pharmaci...|[twitter pharmaci...|(92859,[8,2121,34...|(92859,[8,2121,34...|(92859,[8,2121,34...|         0|\n",
            "|04-03-2020|Anyone been in a ...|anyone supermarke...|[anyone, supermar...|[anyone supermark...|[anyone supermark...|[anyone supermark...|(92859,[61,679,76...|(92859,[61,679,76...|(92859,[61,679,76...|         0|\n",
            "|04-03-2020|Best quality couc...|best quality couc...|[best, quality, c...|[best quality, qu...|[best quality cou...|[best quality, qu...|(92859,[8260,8453...|(92859,[8260,8453...|(92859,[8260,8453...|         0|\n",
            "|04-03-2020|Beware of counter...|beware counterfei...|[beware, counterf...|[beware counterfe...|[beware counterfe...|[beware counterfe...|(92859,[2260,2833...|(92859,[2260,2833...|(92859,[2260,2833...|         0|\n",
            "|04-03-2020|Panic food buying...|panic food buying...|[panic, food, buy...|[panic food, food...|[panic food buyin...|[panic food, food...|(92859,[4,139,113...|(92859,[4,139,113...|(92859,[4,139,113...|         0|\n",
            "|04-03-2020|#Covid_19 Went to...|went grocery stor...|[went, grocery, s...|[went grocery, gr...|[went grocery sto...|[went grocery, gr...|(92859,[0,8,17,20...|(92859,[0,8,17,20...|(92859,[0,8,17,20...|         0|\n",
            "|04-03-2020|While we were bus...|busy watching ele...|[busy, watching, ...|[busy watching, w...|[busy watching el...|[busy watching, w...|(92859,[1365,9458...|(92859,[1365,9458...|(92859,[1365,9458...|         0|\n",
            "|04-03-2020|#AirSewa \r\r\n",
            "\r\r\n",
            "@f...|airsewa providing...|[airsewa, providi...|[airsewa providin...|[airsewa providin...|[airsewa providin...|(92859,[2636,3743...|(92859,[2636,3743...|(92859,[2636,3743...|         0|\n",
            "|05-03-2020|What Precautionar...|precautionary mea...|[precautionary, m...|[precautionary me...|[precautionary me...|[precautionary me...|(92859,[1282,2315...|(92859,[1282,2315...|(92859,[1282,2315...|         0|\n",
            "|05-03-2020|When youre stock...|stockpiling food ...|[stockpiling, foo...|[stockpiling food...|[stockpiling food...|[stockpiling food...|(92859,[6,7,32,47...|(92859,[6,7,32,47...|(92859,[6,7,32,47...|         0|\n",
            "|05-03-2020|That's about a we...|week optimistic p...|[week, optimistic...|[week optimistic,...|[week optimistic ...|[week optimistic,...|(92859,[38,291,69...|(92859,[38,291,69...|(92859,[38,291,69...|         0|\n",
            "|05-03-2020|Studies show the ...|study show corona...|[study, show, cor...|[study show, show...|[study show coron...|[study show, show...|(92859,[1246,4332...|(92859,[1246,4332...|(92859,[1246,4332...|         0|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDPv59OSveYQ"
      },
      "source": [
        "clustersTable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAfSfhimvjdE",
        "outputId": "160ddab2-56ab-4f1d-dc92-faafd6ec5f9f"
      },
      "source": [
        "topbigrams = clustersTable.withColumn('hot_2_gram', F.explode(F.col('features')))\\\n",
        "    .where('prediction = 0') \\\n",
        "    .groupBy('hot_2_gram')\\\n",
        "    .count()\\\n",
        "    .sort('count', ascending=False)\\\n",
        "    .select('hot_2_gram')\n",
        "topbigrams.show(10)\n",
        "['grocery store','toilet paper','stock food','online shopping','panic buying','retail store']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+\n",
            "|     hot_2_gram|\n",
            "+---------------+\n",
            "|  grocery store|\n",
            "|   toilet paper|\n",
            "|     stock food|\n",
            "|online shopping|\n",
            "|   panic buying|\n",
            "|   retail store|\n",
            "|      food bank|\n",
            "|    food supply|\n",
            "| hand sanitizer|\n",
            "|shopping online|\n",
            "+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edLbP1qJv6I7",
        "outputId": "8b13b386-075a-455b-b0a9-b191e53473c3"
      },
      "source": [
        "topbigrams = clustersTable.withColumn('hot_2_gram', F.explode(F.col('features')))\\\n",
        "    .where('prediction = 1') \\\n",
        "    .groupBy('hot_2_gram')\\\n",
        "    .count()\\\n",
        "    .sort('count', ascending=False)\\\n",
        "    .select('hot_2_gram')\n",
        "topbigrams.rdd.map(lambda x : x).take(10)\n",
        "['grocery store','going grocery','store like','grocery store like','tuna cotton','cotton candy']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(hot_2_gram='grocery store'),\n",
              " Row(hot_2_gram='going grocery'),\n",
              " Row(hot_2_gram='going grocery store'),\n",
              " Row(hot_2_gram='store like'),\n",
              " Row(hot_2_gram='grocery store like'),\n",
              " Row(hot_2_gram='tuna cotton'),\n",
              " Row(hot_2_gram='instacart thestruggleisreal coronavirusinla'),\n",
              " Row(hot_2_gram='cotton candy'),\n",
              " Row(hot_2_gram='grocery store coffee'),\n",
              " Row(hot_2_gram='fight club panicbuying')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyQ-TVifv-a0",
        "outputId": "ea596c53-fba4-46b6-e2d9-a21d4816508b"
      },
      "source": [
        "topbigrams = clustersTable.withColumn('hot_2_gram', F.explode(F.col('features')))\\\n",
        "    .where('prediction = 2') \\\n",
        "    .groupBy('hot_2_gram')\\\n",
        "    .count()\\\n",
        "    .sort('count', ascending=False)\\\n",
        "    .select('hot_2_gram')\n",
        "topbigrams.rdd.map(lambda x : x).take(10)\n",
        "['update coronavirus','around shopping either','offering drive around','update coronavirus offering','online class','around shopping']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(hot_2_gram='update coronavirus'),\n",
              " Row(hot_2_gram='around shopping either'),\n",
              " Row(hot_2_gram='offering drive around'),\n",
              " Row(hot_2_gram='update coronavirus offering'),\n",
              " Row(hot_2_gram='online class'),\n",
              " Row(hot_2_gram='around shopping'),\n",
              " Row(hot_2_gram='kid offering'),\n",
              " Row(hot_2_gram='class kid offering'),\n",
              " Row(hot_2_gram='free educational online'),\n",
              " Row(hot_2_gram='job around')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4xaA_z_0OAF",
        "outputId": "b720b9d0-7de5-4d78-bd69-03d53ee3233e"
      },
      "source": [
        "topbigrams = clustersTable.withColumn('hot_2_gram', F.explode(F.col('features')))\\\n",
        "    .where('prediction = 3') \\\n",
        "    .groupBy('hot_2_gram')\\\n",
        "    .count()\\\n",
        "    .sort('count', ascending=False)\\\n",
        "    .select('hot_2_gram')\n",
        "topbigrams.rdd.map(lambda x : x).take(10)\n",
        "['freak almost impossibly','netflix movie','impossibly relevant shut','shut time','shut time also','survived trip']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(hot_2_gram='freak almost impossibly'),\n",
              " Row(hot_2_gram='netflix movie'),\n",
              " Row(hot_2_gram='impossibly relevant shut'),\n",
              " Row(hot_2_gram='shut time'),\n",
              " Row(hot_2_gram='shut time also'),\n",
              " Row(hot_2_gram='survived trip'),\n",
              " Row(hot_2_gram='supermarket daughter'),\n",
              " Row(hot_2_gram='relevant shut time'),\n",
              " Row(hot_2_gram='impossibly relevant'),\n",
              " Row(hot_2_gram='half netflix movie')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euWHPduq0pam",
        "outputId": "7f8c7398-3213-49b5-fe10-a18c650487eb"
      },
      "source": [
        "topbigrams = clustersTable.withColumn('hot_2_gram', F.explode(F.col('features')))\\\n",
        "    .where('prediction = 4') \\\n",
        "    .groupBy('hot_2_gram')\\\n",
        "    .count()\\\n",
        "    .sort('count', ascending=False)\\\n",
        "    .select('hot_2_gram')\n",
        "topbigrams.rdd.map(lambda x : x).take(10)\n",
        "['supply including','cleaning supply sure','stock supply','medical supply','including food','risk outbreak']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(hot_2_gram='supply including'),\n",
              " Row(hot_2_gram='cleaning supply sure'),\n",
              " Row(hot_2_gram='stock supply'),\n",
              " Row(hot_2_gram='medical supply'),\n",
              " Row(hot_2_gram='including food'),\n",
              " Row(hot_2_gram='risk outbreak'),\n",
              " Row(hot_2_gram='medical supply treat'),\n",
              " Row(hot_2_gram='supply treat'),\n",
              " Row(hot_2_gram='considered risk outbreak'),\n",
              " Row(hot_2_gram='supply including food')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-hF6nh7K6xm"
      },
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from pymongo import MongoClient\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPmBQuokLraJ"
      },
      "source": [
        "def read_from_mongodb():\n",
        "    mongoClient = MongoClient(\"mongodb://hkust:hkustAb$13gid@52.229.166.95:27017/\")\n",
        "    db = mongoClient[\"MSBD5003\"]\n",
        "    collection = db.tweets\n",
        "    data = pd.DataFrame(list(collection.find().limit(100)))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "91XozyGNJHkW",
        "outputId": "7fc91cc6-04b9-4828-e8ba-95ee558799a5"
      },
      "source": [
        "data = read_from_mongodb()\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>created</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5faa5120197b1f2104347d22</td>\n",
              "      <td>1244051645975191557</td>\n",
              "      <td>“People are just storing up. They are staying ...</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5faa5120197b1f2104347d23</td>\n",
              "      <td>1244051646935633921</td>\n",
              "      <td>.@PatriceHarrisMD spoke with @YahooFinance abo...</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5faa5120197b1f2104347d24</td>\n",
              "      <td>1244051645971025920</td>\n",
              "      <td>First medical team aiding #Wuhan in fight agai...</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5faa5120197b1f2104347d25</td>\n",
              "      <td>1244051647149543426</td>\n",
              "      <td>.@KathyGriffin: @realDonaldTrump Is 'Lying' Ab...</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5faa5120197b1f2104347d26</td>\n",
              "      <td>1244051645102579712</td>\n",
              "      <td>#CoronaUpdate | Johns Hopkins University has s...</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        _id  ...               created\n",
              "0  5faa5120197b1f2104347d22  ...  2020-03-29T00:00:00Z\n",
              "1  5faa5120197b1f2104347d23  ...  2020-03-29T00:00:00Z\n",
              "2  5faa5120197b1f2104347d24  ...  2020-03-29T00:00:00Z\n",
              "3  5faa5120197b1f2104347d25  ...  2020-03-29T00:00:00Z\n",
              "4  5faa5120197b1f2104347d26  ...  2020-03-29T00:00:00Z\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ8OztJEJL64",
        "outputId": "a7f7471d-0d7d-4f77-9881-bcf95b929c19"
      },
      "source": [
        "data['_id'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectId('5faa5120197b1f2104347d22')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "pO6cPBlVJR9D",
        "outputId": "367f6e9d-fec3-4ced-b54f-cac434fa826d"
      },
      "source": [
        "tt1 = spark.createDataFrame(data)\n",
        "tt1.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'bson.objectid.ObjectId'>",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-056b785df84b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    346\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    347\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    346\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    347\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"not supported type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: not supported type: <class 'bson.objectid.ObjectId'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy4YLdPQJs7E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}